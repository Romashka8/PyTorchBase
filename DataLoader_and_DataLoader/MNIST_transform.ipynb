{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ca7743-66f2-465d-85a7-c834dfa05cba",
   "metadata": {},
   "source": [
    "### Transform MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae537e8f-393a-4251-972d-dd7ae03ea5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e26b0ffc-8625-4649-bec5-7c614035ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "654ff471-7807-4112-a43e-79c737b90ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1468208-8a1e-4662-aed9-41aebb53cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our MNIST dataset implementation\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.len_dataset = 0\n",
    "        self.data_list = []\n",
    "\n",
    "        for path_dir, dir_list, file_list in os.walk(path):\n",
    "            if path_dir == path:\n",
    "                self.classes = sorted(dir_list)\n",
    "                # dict with class names and they positions in one_hot vector\n",
    "                self.class_to_index = {\n",
    "                    cls_name: i for i, cls_name in enumerate(self.classes)\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            cls = path_dir.split('/')[-1]\n",
    "\n",
    "            for name_file in file_list:\n",
    "                file_path = os.path.join(path_dir, name_file)\n",
    "                self.data_list.append((file_path, self.class_to_index[cls]))\n",
    "\n",
    "            self.len_dataset += len(file_list)\n",
    "\n",
    "    # always implement length of dataset here\n",
    "    def __len__(self):\n",
    "        return self.len_dataset\n",
    "\n",
    "    # data with they class\n",
    "    def __getitem__(self, index):\n",
    "        file_path, target = self.data_list[index]\n",
    "        sample = np.array(Image.open(file_path))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        # return sample and it's class position in one_hot vector\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37236048-9857-402e-b2d9-6815f1c4b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        # norm only one color channel\n",
    "        v2.Normalize(mean=(0.5, ), std=(0.5, ))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e641a7-c1b7-4dcc-84ab-3453924a4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "path = os.path.join(os.getcwd(), 'mnist')\n",
    "train_data = MNISTDataset(os.path.join(path, 'training'), transform=transform)\n",
    "test_data = MNISTDataset(os.path.join(path, 'testing'), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e013ff24-ce04-421c-abfc-a184c194a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img:\n",
      "    <class 'torchvision.tv_tensors._image.Image'>\n",
      "    torch.Size([1, 28, 28])\n",
      "    torch.float32\n",
      "    min = -1.0, max = 1.0\n",
      "cls:\n",
      "    2\n"
     ]
    }
   ],
   "source": [
    "img, cls = test_data[2]\n",
    "\n",
    "print('Img:')\n",
    "print(f'    {type(img)}')\n",
    "print(f'    {img.shape}')\n",
    "print(f'    {img.dtype}')\n",
    "print(f'    min = {img.min()}, max = {img.max()}')\n",
    "print('cls:')\n",
    "print(f'    {cls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d42663-cdc3-4a7b-86c2-55340d3c69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "train_data, val_data = random_split(train_data, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5194b75-793c-420b-93eb-9e5760ac2227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do batches\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3198eb1-31e3-4c24-9a93-873ea2fea7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imgs:\n",
      "    <class 'torch.Tensor'>\n",
      "    torch.Size([16, 1, 28, 28])\n",
      "    torch.float32\n",
      "    min = -1.0, max = 1.0\n",
      "cls:\n",
      "    <class 'torch.Tensor'>\n",
      "    torch.Size([16])\n",
      "    torch.int64\n"
     ]
    }
   ],
   "source": [
    "imgs, cls = next(iter(train_loader))\n",
    "\n",
    "print('Imgs:')\n",
    "print(f'    {type(imgs)}')\n",
    "print(f'    {imgs.shape}')\n",
    "print(f'    {imgs.dtype}')\n",
    "print(f'    min = {img.min()}, max = {img.max()}')\n",
    "print('cls:')\n",
    "print(f'    {type(cls)}')\n",
    "print(f'    {cls.shape}')\n",
    "print(f'    {cls.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46512d-f369-469a-8339-da5becc8650b",
   "metadata": {},
   "source": [
    "#### Do the same with ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4992c68b-588c-4e27-b71e-8037cbfb6fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageFolder always create 3 color channels, so add one more transformation\n",
    "transform = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        v2.Grayscale(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=(0.5, ), std=(0.5, ))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11c7929-cc81-4482-8ea2-9da8f617e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), 'mnist')\n",
    "train_data = ImageFolder(os.path.join(path, 'training'), transform=transform)\n",
    "test_data = ImageFolder(os.path.join(path, 'testing'), transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8160ed1c-cfe3-4ef6-8ce1-67983e9b383e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Img:\n",
      "    <class 'torchvision.tv_tensors._image.Image'>\n",
      "    torch.Size([1, 28, 28])\n",
      "    torch.float32\n",
      "    min = -1.0, max = 0.992156982421875\n",
      "cls:\n",
      "    0\n"
     ]
    }
   ],
   "source": [
    "img, cls = test_data[2]\n",
    "\n",
    "print('Img:')\n",
    "print(f'    {type(img)}')\n",
    "print(f'    {img.shape}')\n",
    "print(f'    {img.dtype}')\n",
    "print(f'    min = {img.min()}, max = {img.max()}')\n",
    "print('cls:')\n",
    "print(f'    {cls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652fa809-bd27-40f4-aad6-57dab68fec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = random_split(train_data, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bce79a3-10d8-4e9e-acd7-0a4e9f4438fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b89e5e0-706c-42ff-9647-967780871310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imgs:\n",
      "    <class 'torch.Tensor'>\n",
      "    torch.Size([16, 1, 28, 28])\n",
      "    torch.float32\n",
      "    min = -1.0, max = 0.992156982421875\n",
      "cls:\n",
      "    <class 'torch.Tensor'>\n",
      "    torch.Size([16])\n",
      "    torch.int64\n"
     ]
    }
   ],
   "source": [
    "imgs, cls = next(iter(train_loader))\n",
    "\n",
    "print('Imgs:')\n",
    "print(f'    {type(imgs)}')\n",
    "print(f'    {imgs.shape}')\n",
    "print(f'    {imgs.dtype}')\n",
    "print(f'    min = {img.min()}, max = {img.max()}')\n",
    "print('cls:')\n",
    "print(f'    {type(cls)}')\n",
    "print(f'    {cls.shape}')\n",
    "print(f'    {cls.dtype}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
